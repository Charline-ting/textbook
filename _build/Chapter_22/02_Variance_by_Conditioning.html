---
redirect_from:
  - "/chapter-22/02-variance-by-conditioning"
interact_link: content/Chapter_22/02_Variance_by_Conditioning.ipynb
kernel_name: python3
kernel_path: content/Chapter_22
has_widgets: false
title: |-
  Variance by Conditioning
pagenum: 112
prev_page:
  url: /Chapter_22/01_Conditional_Expectation_Projection.html
next_page:
  url: /Chapter_22/03_Examples.html
suffix: .ipynb
search: y x e var variance b dw mid expectation big db dy decomposition conditional conditioning random because variable its variability strips black line point equal length segment constant function variances align vertical iteration allows us tools well recall notation previous section jointly distributed variables define where deviation graph below level dark blue generic scatter plot distance sum lengths purple green means hence uncorrelated lets take closer three shifting doesnt affect finally begin end called given denoted value values strip observations above commonly written plus makes sense quantities right hand involved calculation components rough size within individual between measured centers show

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Variance by Conditioning</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Variance-by-Conditioning">Variance by Conditioning<a class="anchor-link" href="#Variance-by-Conditioning"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Iteration allows us to find expectation by conditioning. We now have the tools to find variance by conditioning as well.</p>
<p>Recall the notation of the previous section:</p>
<ul>
<li>$X$ and $Y$ are jointly distributed random variables</li>
<li>$b(X) = E(Y \mid X)$</li>
<li>$D_w = Y - b(X)$</li>
</ul>
<p>Define $D_Y = Y - E(Y)$. Then</p>
$$
D_Y ~ = ~  D_w + (b(X) - E(Y)) ~ = ~ D_w + D_b
$$<p>where $D_b = b(X) - E(Y)$ is the deviation of the random variable $b(X)$ from its expectation $E(Y)$.</p>
<p>In the graph below, the black line is at the level $E(Y)$, and the dark blue point is a generic point $(X, Y)$ in the scatter plot. Its distance from the black line is $D_Y$ and is equal to the sum of two lengths:</p>
<ul>
<li>$D_w$, the length of the purple segment</li>
<li>$D_b$, the length of the green segment</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_22/02_Variance_by_Conditioning_2_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decomposition-of-Variance">Decomposition of Variance<a class="anchor-link" href="#Decomposition-of-Variance"> </a></h3><p>The expectation $E(Y)$ is a constant. That means $D_b = b(X) - E(Y)$ is a function of $X$ and hence is uncorrelated with $D_w$. Because $D_Y = D_w + D_b$, we have a <em>decomposition of variance</em>:</p>
$$
Var(D_Y) ~ = ~ Var(D_w) + Var(D_b)
$$<p>Let's take a closer look at these three variances. Shifting a random variable by a constant doesn't affect variance. So:</p>
<ul>
<li>$Var(D_Y) = Var(Y - E(Y)) = Var(Y)$</li>
<li>$Var(D_b) = Var(b(X) - E(Y)) = Var(b(X))$, the <em>variance of the conditional expectation</em>.</li>
</ul>
<p>Finally, because $E(D_w) = 0$,</p>
$$
\begin{align*}
Var(D_w) ~ &amp;= ~ E(D_w^2) \\
&amp;= ~ E\big{(} (Y - b(X))^2 \big{)} \\
&amp;= ~ E\big{(} E\big{(} (Y - b(X))^2 \mid X \big{)} \big{)}
\end{align*}
$$<p>Because $b(X) = E(Y \mid X)$, the random variable $E\big{(} (Y - b(X))^2 \mid X \big{)}$ is a function of $X$ called the <em>conditional variance of $Y$ given $X$</em> and denoted $Var(Y \mid X)$. Its value at $x$ is $Var(Y \mid X=x)$, that is, the variance of the values of $Y$ in the vertical strip at $x$.</p>
<p>So $Var(D_w) = E(Var(Y \mid X))$ is the <em>expectation of the conditional variance</em>.</p>
<p>Because of these observations, the variance decomposition above is more commonly written as a decomposition of the variance of $Y$:</p>
$$
Var(Y) ~ = ~ E(Var(Y \mid X)) + Var(E(Y \mid X))
$$<p>That is, <strong>the variance is equal to the expectation of the conditional variance plus the variance of the conditional expectation</strong>.</p>
<p>It makes sense that the two quantities on the right hand side are involved in the calculation of $Var(Y)$. The variability of $Y$ has two components:</p>
<ul>
<li>the rough size of the variability within the individual vertical strips, that is, the expectation of the conditional variance</li>
<li>the variability between strips, measured by the variance of the centers of the strips.</li>
</ul>
<p>The variance decomposition show that you can just add the two terms to get $Var(Y)$.</p>
<p>This decomposition is the basis of <em>analysis of variance</em> (ANOVA), widely used in statistics. In this course we are going to use it to find variances by conditioning.</p>

</div>
</div>
</div>
</div>

 


    </main>
    