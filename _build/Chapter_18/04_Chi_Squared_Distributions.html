---
redirect_from:
  - "/chapter-18/04-chi-squared-distributions"
interact_link: content/Chapter_18/04_Chi_Squared_Distributions.ipynb
kernel_name: python3
kernel_path: content/Chapter_18
has_widgets: false
title: |-
  Chi-Squared Distributions
pagenum: 95
prev_page:
  url: /Chapter_18/03_The_Gamma_Family.html
next_page:
  url: /Chapter_18/05_Review_Problems_Set_4.html
suffix: .ipynb
search: n x squared chi frac z distribution gamma freedom mu sigma degrees t normal variable sum estimate standard v density e exponential xi w random variables ldots known bar let sqrt independent d why same zn mean r lambda sd xn zi since not densities thats also called abbreviate different rate parameter showed exercises here variance define linear unbiased case above s xis distributions change formula found fv pi degree establishing properties discovered saw comparing settings rayleigh arises wasnt particularly illuminating reason should shape adds remains therefore explains squares induction cdots positive integer fx graphs through because three names thus

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Chi-Squared Distributions</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Chi-Squared-Distributions">Chi-Squared Distributions<a class="anchor-link" href="#Chi-Squared-Distributions"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $Z$ be a standard normal random variable and let $V = Z^2$. By the change of variable formula for densities, we found the density of $V$ to be</p>
$$
f_V(v) ~ = ~ \frac{1}{\sqrt{2\pi}} v^{-\frac{1}{2}} e^{-\frac{1}{2} v}, ~~~~ v &gt; 0
$$<p>That's the gamma $(1/2, 1/2)$ density. It is also called the <em>chi-squared density with 1 degree of freedom,</em> which we will abbreviate to chi-squared (1).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="From-Chi-Squared-$(1)$-to-Chi-Squared-$(n)$">From Chi-Squared $(1)$ to Chi-Squared $(n)$<a class="anchor-link" href="#From-Chi-Squared-$(1)$-to-Chi-Squared-$(n)$"> </a></h3><p>When we were establishing the properties of the standard normal density, we discovered that if $Z_1$ and $Z_2$ are independent standard normal then $Z_1^2 + Z_2^2$ has the exponential $(1/2)$ distribution. We saw this by comparing two different settings in which the Rayleigh distribution arises. But that wasn't a particularly illuminating reason for why $Z_1^2 + Z_2^2$ should be exponential.</p>
<p>But now we know that the sum of independent gamma variables with the same rate is also gamma; the shape parameter adds up and the rate remains the same. Therefore $Z_1^2 + Z_2^2$ is a gamma $(1, 1/2)$ variable. That's the same distribution as exponential $(1/2)$, as you showed in exercises. This explains why the sum of squares of two i.i.d. standard normal variables has the exponential $(1/2)$ distribution.</p>
<p>Now let $Z_1, Z_2, \ldots, Z_n$ be i.i.d. standard normal variables. Then $Z_1^2, Z_2^2, \ldots, Z_n^2$ are i.i.d. chi-squared $(1)$ variables. That is, each of them has the gamma $(1/2, 1/2)$ distribution.</p>
<p>By induction, $Z_1^2 + Z_2^2 + \cdots + Z_n^2$ has the gamma $(n/2, 1/2)$ distribution. This is called the <em>chi-squared distribution with $n$ degrees of freedom,</em> which we will abbreviate to chi-squared $(n)$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chi-Squared-with-$n$-Degrees-of-Freedom">Chi-Squared with $n$ Degrees of Freedom<a class="anchor-link" href="#Chi-Squared-with-$n$-Degrees-of-Freedom"> </a></h3><p>For a positive integer $n$, the random variable $X$ has the <em>chi-squared distribution with $n$ degrees of freedom</em> if the distribution of $X$ is gamma $(n/2, 1/2)$. That is, $X$ has density</p>
$$
f_X(x) ~ = ~ \frac{\frac{1}{2}^{\frac{n}{2}}}{\Gamma(\frac{n}{2})} x^{\frac{n}{2} - 1} e^{-\frac{1}{2}x}, ~~~~ x &gt; 0
$$<p>Here are the graphs of the chi-squared densities for degrees of freedom 2 through 5.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_18/04_Chi_Squared_Distributions_4_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The chi-squared (2) distribution is exponential because it is the gamma $(1, 1/2)$ distribution. This distribution has three names:</p>
<ul>
<li>chi-squared (2)</li>
<li>gamma (1, 1/2)</li>
<li>exponential (1/2)</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mean-and-Variance">Mean and Variance<a class="anchor-link" href="#Mean-and-Variance"> </a></h3><p>You know that if $T$ has the gamma $(r, \lambda)$ density then</p>
$$
E(T) ~ = ~ \frac{r}{\lambda} ~~~~~~~~~~~~ SD(T) = \frac{\sqrt{r}}{\lambda}
$$<p>If $X$ has the chi-squared $(n)$ distribution then $X$ is gamma $(n/2, 1/2)$. So</p>
$$
E(X) ~ = ~ \frac{n/2}{1/2} ~ = ~ n
$$<p>Thus <strong>the expectation of a chi-squared random variable is its degrees of freedom</strong>.</p>
<p>The SD is
$$
SD(X) ~ = ~ \frac{\sqrt{n/2}}{1/2} ~ = ~ \sqrt{2n}
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Estimating-the-Normal-Variance">Estimating the Normal Variance<a class="anchor-link" href="#Estimating-the-Normal-Variance"> </a></h3><p>Suppose $X_1, X_2, \ldots, X_n$ are i.i.d. normal $(\mu, \sigma^2)$ variables, and that you are in a setting in which you know $\mu$ and are trying to estimate $\sigma^2$.</p>
<p>Let $Z_i$ be $X_i$ in standard units, so that $Z_i = (X_i - \mu)/\sigma$. Define the random variable $T$ as follows:</p>
$$
T ~ = ~ \sum_{i=1}^n Z_i^2 ~ = ~ \frac{1}{\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
$$<p>Then $T$ has the chi-squared $(n)$ distribution and $E(T) = n$. Now define $W$ by</p>
$$
W ~ = ~  \frac{\sigma^2}{n} T ~ = ~ \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2
$$<p>Then $W$ can be computed based on the sample since $\mu$ is known. And since $W$ is a linear tranformation of $T$ it is easy to see that $E(W) = \sigma^2$.</p>
<p>So we have constructed an unbiased estimate of $\sigma^2$. It is the mean squared deviation from the known population mean.</p>
<p>But typically, $\mu$ is not known. In that case you need a different estimate of $\sigma^2$ since you can't compute $W$ as defined above. You showed in exercises that</p>
$$
S^2 ~ = ~ \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2
$$<p>is an unbiased estimate of $\sigma^2$ regardless of the distribution of the $X_i$'s. When the $X_i$'s are normal, as is the case here, it turns out that $S^2$ is a linear transformation of a chi-squared $(n-1)$ random variable. The methods of the next chapter can used to understand why.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="&quot;Degrees-of-Freedom&quot;">"Degrees of Freedom"<a class="anchor-link" href="#&quot;Degrees-of-Freedom&quot;"> </a></h3><p>The example above helps explain the strange term "degrees of freedom" for the parameter of the chi-squared distribution.</p>
<ul>
<li>When $\mu$ is known, you have $n$ independent centered normals $(X_i - \mu)$ that you can use to estimate $\sigma^2$. That is, you have $n$ degrees of freedom in constructing your estimate.</li>
<li>When $\mu$ is not known, you are using all $n$ of $X_1 - \bar{X}, X_2 - \bar{X}, \ldots, X_n - \bar{X}$ in your estimate, but they are not independent. They are the deviations of the list $X_1, X_2, \ldots , X_n$ from their average $\bar{X}$, and hence their sum is 0. If you know $n-1$ of them, the final one is determined. So you only have $n-1$ degrees of freedom.</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    