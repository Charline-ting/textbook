---
redirect_from:
  - "/chapter-12/03-bounds"
interact_link: content/Chapter_12/03_Bounds.ipynb
kernel_name: python3
kernel_path: content/Chapter_12
has_widgets: false
title: |-
  Tail Bounds
pagenum: 62
prev_page:
  url: /Chapter_12/02_Prediction_and_Estimation.html
next_page:
  url: /Chapter_12/04_Heavy_Tails.html
suffix: .ipynb
search: x c ge mux p e z sigmax g h inequality le bound k variable frac distribution random markovs big chebyshevs sd omega graph non negative units probability text function above mean tail gold area below circ sds matter standard vert bounds get tails functions thus figure quite think least not follows going upper probabilities such suppose result let positive indicator straight line just called because says pretty another expectation chance times tell does far better outside interval either often convenient origin measure distances align assumptions shape bulk good setting its same values range idea much section thats whose displayed

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Tail Bounds</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tail-Bounds">Tail Bounds<a class="anchor-link" href="#Tail-Bounds"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you know $E(X)$ and $SD(X)$ you can get some idea of how much probability there is in the tails of the distribution of $X$.</p>
<p>In this section we are going to get upper bounds on probabilities such as the gold area in the graph below. That's $P(X \ge 20)$ for the random variable $X$ whose distribution is displayed in the histogram.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_12/03_Bounds_2_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Monotonicity">Monotonicity<a class="anchor-link" href="#Monotonicity"> </a></h3><p>To do this, we will start with an observation about expectations of functions of $X$.</p>
<p>Suppose $g$ and $h$ are functions such that $g(X) \ge h(X)$, that is, $P(g(X) \ge h(X)) = 1$. Then $E(g(X)) \ge E(h(X))$.</p>
<p>This result is apparent when you notice that for all $\omega$ in the outcome space,</p>
$$
(g \circ X)(\omega) \ge (h \circ X)(\omega) ~~~~ \text{and therefore} ~~~~
(g \circ X)(\omega)P(\omega) \ge (h \circ X)(\omega)P(\omega)
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now suppose $X$ is a non-negative random variable, and let $c$ be a positive number. Consider the two functions $g$ and $h$ graphed below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_12/03_Bounds_5_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The function $h$ is the indicator defined by $h(x) = I(x \ge c)$. So $h(X) = I(X \ge c)$ and $E(h(X)) = P(X \ge c)$.</p>
<p>The function $g$ is constructed so that the graph of $g$ is a straight line that is at or above the graph of $h$ on $[0, \infty)$, with the two graphs meeting at $x = 0$ and $x = c$. The equation of the straight line is $g(x) = x/c$.</p>
<p>Thus $g(X) = X/c$ and hence $E(g(X)) = E(X/c) = E(X)/c$.</p>
<p>By construction, $g(x) \ge h(x)$ for $x \ge 0$. Since $X$ is a non-negative random variable, $P(g(X) \ge h(X)) = 1$.</p>
<p>So</p>
$$
E(X)/c ~ = ~ E(g(X)) ~ \ge ~ E(h(X)) ~ =  ~ P(X \ge c)
$$<p>We have just proved</p>
<h3 id="Markov's-Inequality">Markov's Inequality<a class="anchor-link" href="#Markov's-Inequality"> </a></h3><p>Let $X$ be a non-negative random variable. Then for any $c &gt; 0$,</p>
$$
P(X \ge c) ~ \le ~ \frac{E(X)}{c}
$$<p>This result is called a "tail bound" because it puts an upper limit on how big the right tail at $c$ can be. It is worth noting that $P(X &gt; c) \le P(X \ge c) \le E(X)/c$ by Markov's bound.</p>
<p>In the figure below, $E(X) = 6.5$ and $c = 20$. Markov's inequality says that the gold area is <em>at most</em></p>
$$
\frac{6.5}{20} = 0.325
$$<p>You can see that the bound is pretty crude. The gold area is clearly quite a bit less than 0.325.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_12/03_Bounds_7_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another way to think of Markov's bound is that if $X$ is a non-negative random variable with expectation $\mu_X$, then</p>
$$
P(X \ge k\mu_X) ~ \le ~ \frac{1}{k} ~~~ \text{for all } k &gt; 0
$$<p>That is, $P(X \ge 2\mu_X) \le 1/2$, $P(X \ge 5\mu_X) \le 1/5$, and so on. The chance that a non-negative random variable is at least $k$ times the mean is at most $1/k$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notes:</p>
<ul>
<li>$k$ need not be an integer. For example, the chance that a non-negative random variable is at least 3.8 times the mean is at most $1/3.8$.</li>
<li>If $k \le 1$, the inequality doesn't tell you anything you didn't already know. If $k \le 1$ then Markov's bound is 1 or greater. All probabilities are bounded above by 1, so the inequality is true but useless for $k \le 1$. </li>
<li>When $k$ is large, the bound does tell you something. You are looking at a probability quite far out in the tail of the distribution, and Markov's bound is $1/k$ which is small.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chebyshev's-Inequality">Chebyshev's Inequality<a class="anchor-link" href="#Chebyshev's-Inequality"> </a></h3><p>Markov's bound only uses $E(X)$, not $SD(X)$. To get bounds on tails it seems better to use $SD(X)$ if we can. <em>Chebyshev's Inequality</em> does just that. It provides a bound on the two tails outside an interval that is symmetric about $E(X)$ as in the following graph.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_12/03_Bounds_11_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The red arrow marks $\mu_X$ as usual, and now the two blue arrows are at a distance of $SD(X)$ on either side of the mean. It is often going to be convenient to think of $E(X)$ as "the origin" and to measure distances in units of SDs on either side.</p>
<p>Thus we can think of the gold area as the probability that $X$ is at least $z$ SDs away from $\mu_X$, for some positive $z$. Now</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{align*}
P\big{(}|X - \mu_X| \ge z\sigma_X\big{)} &amp;= P\big{(}(X-\mu_X)^2 \ge z^2\sigma_X^2\big{)} \\ \\
&amp;\le \frac{E\big{[}(X-\mu_X)^2\big{]}}{z^2\sigma_X^2} ~~~~~ \text{(Markov's Inequality)}\\ \\
&amp;= \frac{\sigma_X^2}{z^2\sigma_X^2} ~~~~~ \text{(definition of variance)} \\ \\
&amp;= \frac{1}{z^2}
\end{align*}
$$<p>Chebyshev's Inequality makes no assumptions about the shape of the distribution. It implies that no matter what the distribution of $X$ looks like,</p>
<ul>
<li><p>$P(\mu_X - 2\sigma_X &lt; X &lt; \mu_X + 2\sigma_X) &gt; 1 - 1/4 = 75\%$</p>
</li>
<li><p>$P(\mu_X - 3\sigma_X &lt; X &lt; \mu_X + 3\sigma_X) &gt; 1 - 1/9 = 88.88...\%$</p>
</li>
<li>$P(\mu_X - 4\sigma_X &lt; X &lt; \mu_X + 4\sigma_X) &gt; 1 - 1/16 = 93.75\%$</li>
<li>$P(\mu_X - 5\sigma_X &lt; X &lt; \mu_X + 5\sigma_X) &gt; 1 - 1/25 = 96\%$</li>
</ul>
<p>That is, <em>no matter what the shape of the distribution</em>, the bulk of the probability is in the interval "expected value plus or minus a few SDs".</p>
<p>This is one reason why the SD is a good measure of spread. No matter what the distribution, if you know the expectation and the SD then you have a pretty good sense of where the bulk of the probability is located.</p>
<p>If you happen to know more about the distribution then of course you can do better than Chebyshev's bound. But in general Chebyshev's bound is as well as you can do without making further assumptions.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Standard-Units">Standard Units<a class="anchor-link" href="#Standard-Units"> </a></h3><p>To formalize the notion of "setting $\mu_X$ as the origin and measuring distances in units of $\sigma_X$, we define a random variable $Z$ called "$X$ in standard units" as follows:</p>
$$
Z = \frac{X - \mu_X}{\sigma_X}
$$<p>$Z$ measures how far $X$ is above its mean, relative to its SD. In other words, $X$ is $Z$ SDs above the mean:</p>
$$
X = Z\sigma_X + \mu_X
$$<p>It is important to learn to go back and forth between these two scales of measurement, as we will be using standard units quite frequently. Note that by the linear function rules,</p>
$$
E(Z) = 0 ~~~~ \text{and} ~~~~ SD(Z) = 1
$$<p>no matter what the distribution of $X$ is.</p>
<p>Chebyshev's Inequality says</p>
$$
P(|X - \mu_X| \ge z\sigma_X) \le \frac{1}{z^2}
$$<p>which is the same as saying</p>
$$
P(|Z| \ge z) \le \frac{1}{z^2}
$$<p>So if you have converted a random variable to standard units, the overwhelming majority of the values of the standardized variable should be in the range $-5$ to $5$. It is possible that there are values outside that range, but it is not likely.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Another-Way-of-Writing-Chebyshev's-Inequality">Another Way of Writing Chebyshev's Inequality<a class="anchor-link" href="#Another-Way-of-Writing-Chebyshev's-Inequality"> </a></h3><p>Chebyshev's Inequality is often written as follows:</p>
<p>For all $c &gt; 0$,</p>
$$
P\big{(}|X - \mu_X| \ge c \big{)} ~ \le ~ \frac{\sigma_X^2}{c^2}
$$<p>This is the same as our statement but with $c$ replacing $z\sigma_X$. We will use whichever form happens to be convenient in a given setting.</p>
<p>The figure below is analogous to the figure drawn earlier to illustrate the derivation of Markov's inequality.</p>
<p>The graph of the quadratic function $g(x) = (x - \mu_X)^2/c^2$ is always at or above the graph of the indicator function $h(x) = I(\vert x - \mu_X \vert \ge c)$.</p>
<p>Thus $E(g(X)) ~ \ge ~ E(h(X)) ~ = ~ P(\vert X - \mu_X \vert \ge c)$. Chebyshev's inequality follows, because $E(g(X)) ~ = ~ E((X - \mu_X)^2)/c^2 ~ = ~ \sigma_X^2/c^2$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered tag_remove_input">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_12/03_Bounds_16_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

 


    </main>
    