---
redirect_from:
  - "/chapter-25/03-multivariate-normal-conditioning"
interact_link: content/Chapter_25/03_Multivariate_Normal_Conditioning.ipynb
kernel_name: python3
has_widgets: false
title: 'Conditioning and the Multivariate Normal'
prev_page:
  url: /Chapter_25/02_Best_Linear_Predictor.html
  title: 'Best Linear Predictor'
next_page:
  url: /Chapter_25/04_Multiple_Regression.html
  title: 'Multiple Regression'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conditioning-and-the-Multivariate-Normal">Conditioning and the Multivariate Normal<a class="anchor-link" href="#Conditioning-and-the-Multivariate-Normal"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Whe $Y$ and $\mathbf{X}$ have a multivariate normal distribution with positive definite covariance matrix, then best linear predictor derived in the previous section is the best among all predictors of $Y$ based on $\mathbf{X}$. That is,</p>
$$
~E(Y \mid \mathbf{X}) = \boldsymbol{\Sigma}_{Y, \mathbf{X}}\boldsymbol{\Sigma}_\mathbf{X}^{-1} (\mathbf{X} - \boldsymbol{\mu}_\mathbf{X}) + \mu_Y
$$$$
Var(Y \mid \mathbf{X}) = \sigma_Y^2 - \boldsymbol{\Sigma}_{Y, \mathbf{X}}\boldsymbol{\Sigma}_\mathbf{X}^{-1} \boldsymbol{\Sigma}_{\mathbf{X}, Y}
$$<p>Also, the conditional distribution of $Y$ given $\mathbf{X}$ is normal.</p>
<p>These results are extensions of those in the case where $Y$ was predicted based on just one predictor $X$. To prove them, you need some linear algebra and some patience. We won't do the proofs here. Based on what you have seen in the case of a single predictor, it should not be hard to believe that they are true.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For some reassurance, we can simulate data from a trivariate normal distribution and see how our formula for the conditional expectation works in relation to the simulated points.</p>
<p>To do this, we will first set up some notation. When we say that $Y$ and $\mathbf{X}$ have a multivariate normal distribution, we are saying that the $(1+p) \times 1$ random vector $[Y, X_1, X_2, \ldots, X_p]^T$ has a bivariate normal distribution.</p>
<p>To keep our variables organized and our notation compact, we will <em>partition</em> the random vector and its mean vector.</p>
$$
\begin{bmatrix}
Y \\
X_1 \\
X_2 \\
\vdots \\
X_p
\end{bmatrix}
~ = ~ 
\begin{bmatrix}
Y \\
\mathbf{X}
\end{bmatrix}
~~~~~~~~~~~~~~~
\begin{bmatrix}
\mu_Y \\
\mu_{X_1} \\
\mu_{X_2} \\
\vdots \\
\mu_{X_p}
\end{bmatrix}
~ = ~ 
\begin{bmatrix}
\mu_Y \\
\boldsymbol{\mu}_\mathbf{X}
\end{bmatrix}
$$<p>We can partition the covariance matrix as well, according to the demarcating lines shown below.</p>
$$
\boldsymbol{\Sigma} ~ = ~
\left[\begin{array}{c|cccc}
\sigma_Y^2 &amp; \sigma_{Y, X_1} &amp; \sigma_{Y, X_2} &amp; \cdots &amp; \sigma_{Y, X_p}\\ \hline
\sigma_{X_1, Y} &amp; \sigma_{X_1}^2 &amp; \sigma_{X_1, X_3} &amp; \cdots &amp; \sigma_{X_2, X_p} \\ 
\vdots &amp; \vdots &amp; \ddots &amp;\vdots &amp; \vdots \\ 
\sigma_{X_p, Y} &amp; \sigma_{X_p, X_1} &amp; \sigma_{X_p, X_2} &amp; \cdots &amp; \sigma_{X_p}^2 \\
\end{array}\right]
~ = ~ 
\left[\begin{array}{c|c}
\sigma_Y^2&amp; \boldsymbol{\Sigma}_{Y,\mathbf{X}} \\ \hline
\boldsymbol{\Sigma}_{\mathbf{X},Y} &amp; \boldsymbol{\Sigma}_\mathbf{X}\\
\end{array}\right]
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The cell below produces a simulation of 200 points drawn from the multivariate normal distribution with the parameters provided. The variable plotted on the vertical dimension is $Y$, with the other two axes representing the two predictors $X_1$ and $X_2$.</p>
<p>The plane is</p>
$$
E(Y \mid \mathbf{X}) = \boldsymbol{\Sigma}_{Y, \mathbf{X}}\boldsymbol{\Sigma}_\mathbf{X}^{-1} (\mathbf{X} - \boldsymbol{\mu}_\mathbf{X}) + \mu_Y
$$<p></p>
<p><strong>Keep in mind that the plane is computed according to this formula; it has not been estimated based on the simulated points.</strong></p>
<p>Notice that all three variables are in standard units and that the two predictor variables are not highly correlated: $r(X_1, X_2) = 0.2$. You can change the parameters, of course, but you will get an error message if you enter a "covariance matrix" that is not positive semidefinite.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">Plot_multivariate_normal_cond_exp</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_25/03_Multivariate_Normal_Conditioning_4_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the three-dimensional version of the familiar football shaped scatter diagram with the "best predictor" line going through it. The plane that is the conditional expectation of $Y$ given $\mathbf{X}$ goes through the "vertical center" of the cloud.</p>
<p>In the simulation below, the correlations between $Y$ and two predictor variables have been reduced. Notice the greater spread about the plane.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">Plot_multivariate_normal_cond_exp</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_25/03_Multivariate_Normal_Conditioning_6_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The calcuations of this chapter, for predicting the value of a random variabe $Y$ by a linear function of random variables $X_1, X_2, \ldots, X_p$, have direct applications to data.</p>
<p>In the data setting, what we see is just a cloud of points:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Scatter_multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_25/03_Multivariate_Normal_Conditioning_8_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But we don't know the parameters of the distribution, so we can't draw the right plane through the scatter. The problem of multiple regression is to <em>estimate</em> that plane based on the data, under appropriate assumptions.</p>
<p>That is the topic of the next section, which concludes the course.</p>

</div>
</div>
</div>
</div>

 

