---
redirect_from:
  - "/chapter-06/05-law-of-small-numbers"
interact_link: content/Chapter_06/05_Law_of_Small_Numbers.ipynb
kernel_name: python3
kernel_path: content/Chapter_06
has_widgets: false
title: |-
  The Law of Small Numbers
pagenum: 35
prev_page:
  url: /Chapter_06/04_Odds_Ratios.html
next_page:
  url: /Chapter_07/00_Poissonization.html
suffix: .ipynb
search: k pn n mu frac distribution binomial approximation npn poisson large small big sim p successes because probability only values e cdot probabilities let just called here though possible approximating infty such its well log muk parameter terms infinite stats pmf law numbers chance note times why bother computing histogram very should start case value text cant fixed align le expansion get python exact function histograms string label thats parameters consecutive odds ratios help us derive sometimes approximates success expect example pretty natural thinking trials anywhere between probable rather didnt even beyond since scrunched near few bars noticeable really approximate

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The Law of Small Numbers</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Law-of-Small-Numbers">The Law of Small Numbers<a class="anchor-link" href="#The-Law-of-Small-Numbers"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The consecutive odds ratios of the binomial $(n, p)$ distribution help us derive an approximation for the distribution when $n$ is large and $p$ is small. The approximation is sometimes called "the law of small numbers" because it approximates the distribution of the number of successes when the chance of success is small: you only expect a small number of successes.</p>
<p>As an example, here is the binomial $(1000, 2/1000)$ distribution. Note that $1000$ is large, $2/1000$ is pretty small, and $1000 \times (2/1000) = 2$ is the natural number of successes to be thinking about.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span><span class="o">/</span><span class="mi">1000</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="n">binom_probs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">binom_dist</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">probabilities</span><span class="p">(</span><span class="n">binom_probs</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">binom_dist</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_06/05_Law_of_Small_Numbers_2_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Though the possible values of the number of successes in 1000 trials can be anywhere between 0 and 1000, the <em>probable</em> values are all rather small because $p$ is small. That is why we didn't even bother computing the probabilities beyond $k = 15$.</p>
<p>Since the histogram is all scrunched up near 0, only very few bars have noticeable probability. It really should be possible to find or approximate the chances of the corresponding values by a simpler calculation than the binomial formula.</p>
<p>To see how to do this, we will start with $P(0)$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Approximating-$P(0)$">Approximating $P(0)$<a class="anchor-link" href="#Approximating-$P(0)$"> </a></h3><p>Let $n \to \infty$ and $p_n \to 0$ in such a way that $np_n \to \mu &gt; 0$. It's important to ensure that $p_n$ doesn't go to 0 so fast that $np_n \to 0$ as well, because in that case all the probability just gets concentrated at the value 0 when $n$ is large.</p>
<p>Let $P_n(k)$ be the binomial $(n, p_n)$ probability of $k$ successes.</p>
<p>Then</p>
$$
P_n(0) = (1 - p_n)^n = \big{(} 1 - \frac{np_n}{n} \big{)}^n
\to e^{-\mu} ~~~ \text{as } n \to \infty
$$<p>If you can't see the limit directly, appeal to our familiar exponential approxmation:</p>
$$
\log(P_n(0)) = n \log \big{(} 1 - \frac{np_n}{n} \big{)}
= n \cdot \log \big{(} 1 - p_n \big{)} 
\sim n(-p_n)
= -np_n
\sim -\mu
$$<p>when $n$ is large, because $p_n \sim 0$ and $np_n \sim \mu$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Approximating-$P(k)$">Approximating $P(k)$<a class="anchor-link" href="#Approximating-$P(k)$"> </a></h3><p>In general, for fixed $k &gt; 1$,</p>
$$
\begin{align*}
P_n(k) &amp;= P_n(k-1)R_n(k) \\ \\
&amp;= P_n(k-1)\frac{n-k+1}{k} \cdot \frac{p_n}{1-p_n} \\ \\
&amp;= P_n(k-1) \big{(} \frac{np_n}{k} - \frac{(k-1)p_n}{k} \big{)}
\frac{1}{1 - p_n} \\ \\
&amp;\sim P_n(k-1) \cdot \frac{\mu}{k}
\end{align*}
$$<p>when $n$ is large, because $k$ is constant, $np_n \to \mu$, $p_n \to 0$, and $1-p_n \to 1$. By induction, this implies the following approximation for each fixed $k$.</p>
$$
P_n(k) ~ \sim ~ e^{-\mu} \cdot \frac{\mu}{1} \cdot \frac{\mu}{2}
\cdots \frac{\mu}{k}
~ = ~ e^{-\mu} \frac{\mu^k}{k!}
$$<p>if $n$ is large, under all the additional conditions we have assumed. Here is a formal statement.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Poisson-Approximation-to-the-Binomial">Poisson Approximation to the Binomial<a class="anchor-link" href="#Poisson-Approximation-to-the-Binomial"> </a></h3><p>Let $n \to \infty$ and $p_n \to 0$ in such a way that $np_n \to \mu &gt; 0$. Let $P_n(k)$ be the binomial $(n, p_n)$ probability of $k$ successes. Then for each $k$ such that $0 \le k \le n$,</p>
$$
P_n(k) \sim e^{-\mu} \frac{\mu^k}{k!} ~~~
\text{for large } n
$$<p>This is called the Poisson approximation to the binomial. The parameter of the Poisson distribution is $\mu \sim np_n$ for large $n$.</p>
<p>The distribution is named after its originator, the French mathematician <a href="https://en.wikipedia.org/wiki/Siméon_Denis_Poisson">Siméon Denis Poisson</a> (1781-1840).</p>
<p>The terms in the approximation are proportional to the terms in the series expansion of $e^{\mu}$:</p>
$$
\frac{\mu^k}{k!}, ~~ k \ge 0
$$<p>The expansion is infinite, but we are only going up to a finite (though large) number of terms $n$. You now start to see the value of being able to work with probability spaces that have an infinite number of possible outcomes.</p>
<p>We'll get to that in a later section. For now, let's see if the approximation we derived is any good.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Poisson-Probabilities-in-Python">Poisson Probabilities in Python<a class="anchor-link" href="#Poisson-Probabilities-in-Python"> </a></h3><p>Use <code>stats.poisson.pmf</code> just as you would use <code>stats.binomial.pmf</code>, but keep in mind that the Poisson has only one parameter.</p>
<p>Suppose $n = 1000$ and $p = 2/1000$. Then the exact binomial chance of 3 successes is</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.18062773231746918</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The approximating Poisson distribution has parameter $1000 \times (2/1000) = 2$, and so the Poisson approximation to the probability above is</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.18044704431548356</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Not bad. To compare the entire distributions, first create the two distribution objects:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>

<span class="n">bin_probs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">bin_dist</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">probabilities</span><span class="p">(</span><span class="n">bin_probs</span><span class="p">)</span>

<span class="n">poi_probs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">poi_dist</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">probabilities</span><span class="p">(</span><span class="n">poi_probs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>prob140</code> function that draws overlaid histograms is called <code>Plots</code> (note the plural). The syntax has alternating arguments: a string label you provide for a distribution, followed by that distribution, then a string label for the second distribution, then that distribution.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Plots</span><span class="p">(</span><span class="s1">&#39;Binomial (1000, 2/1000)&#39;</span><span class="p">,</span> <span class="n">bin_dist</span><span class="p">,</span> <span class="s1">&#39;Poisson (2)&#39;</span><span class="p">,</span> <span class="n">poi_dist</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_06/05_Law_of_Small_Numbers_14_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Does it look as though there is only one histogram? That's because the approximation is great! Here are the two histograms individually.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Plot</span><span class="p">(</span><span class="n">bin_dist</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_06/05_Law_of_Small_Numbers_16_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Plot</span><span class="p">(</span><span class="n">poi_dist</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/Chapter_06/05_Law_of_Small_Numbers_17_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In lab, you will use total variation distance to get a bound on the error in the approximation.</p>
<p>A reasonable question to ask at this stage is, "Well that's all very nice, but why should I bother with approximations when I can just use Python to compute the exact binomial probabilities using <code>stats.binom.pmf</code>?"</p>
<p>Part of the answer is that if a function involves parameters, you can't understand how it behaves by just computing its values for some particular choices of the parameters. In the case of Poisson probabilities, we will also see shortly that they form a powerful distribution in their own right, on an infinite set of values.</p>

</div>
</div>
</div>
</div>

 


    </main>
    