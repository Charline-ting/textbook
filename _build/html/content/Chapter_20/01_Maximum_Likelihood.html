

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>20.1. Maximum Likelihood## &#8212; Prob 140 Textbook</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/modification.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="20.2. Prior and Posterior" href="02_Prior_and_Posterior.html" />
    <link rel="prev" title="20. Approaches to Estimation" href="00_Approaches_to_Estimation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Prob 140 Textbook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="http://prob140.org">
   Course Home
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../To_the_Student.html">
   To the  Student
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_01/00_Fundamentals.html">
   1. Fundamentals
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/01_Outcome_Space_and_Events.html">
     1.1. Outcome Space and Events
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/02_Equally_Likely_Outcomes.html">
     1.2. Equally Likely Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/03_Collisions_in_Hashing.html">
     1.3. Collisions in Hashing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/04_Birthday_Problem.html">
     1.4. The Birthday Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_01/05_An_Exponential_Approximation.html">
     1.5. An Exponential Approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_02/00_Calculating_Chances.html">
   2. Calculating Chances
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/01_Addition.html">
     2.1. Addition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/02_Examples.html">
     2.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/03_Multiplication.html">
     2.3. Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/04_More_Examples.html">
     2.4. More Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_02/05_Updating_Probabilities.html">
     2.5. Updating Probabilities
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_03/00_Random_Variables.html">
   3. Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/01_Functions_on_an_Outcome_Space.html">
     3.1. Functions on an Outcome Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/02_Distributions.html">
     3.2. Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_03/03_Equality.html">
     3.3. Equality
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_04/00_Relations_Between_Variables.html">
   4. Relations Between Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/01_Joint_Distributions.html">
     4.1. Joint Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/02_Examples.html">
     4.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/03_Marginal_Distributions.html">
     4.3. Marginal Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/04_Conditional_Distributions.html">
     4.4. Conditional Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_04/05_Dependence_and_Independence.html">
     4.5. Dependence and Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_05/00_Collections_of_Events.html">
   5. Collections of Events
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/01_Bounding_the_Chance_of_a_Union.html">
     5.1. Bounding the Chance of a Union
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/02_Inclusion_Exclusion.html">
     5.2. Inclusion-Exclusion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/03_The_Matching_Problem.html">
     5.3. The Matching Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/04_Sampling_Without_Replacement.html">
     5.4. Sampling Without Replacement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_05/05_Review_Problems_Set_1.html">
     5.5. Review Problems: Set 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_06/00_Random_Counts.html">
   6. Random Counts
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/01_Binomial_Distribution.html">
     6.1. The Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/02_Examples.html">
     6.2. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/03_Hypergeometric_Distribution.html">
     6.3. The Hypergeometric Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/04_Odds_Ratios.html">
     6.4. Odds Ratios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_06/05_Law_of_Small_Numbers.html">
     6.5. The Law of Small Numbers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_07/00_Poissonization.html">
   7. Poissonization
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/01_Poissonizing_the_Binomial.html">
     7.3.1. Poissonizing the Binomial
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_07/02_Poissonizing_the_Multinomial.html">
     7.3.2. Poissonizing the Multinomial
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_08/00_Expectation.html">
   8. Expectation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/01_Definition.html">
     8.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/02_Additivity.html">
     8.2. Additivity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/03_Expectations_of_Functions.html">
     8.3. Expectations of Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_08/04_Review_Problems_Set_2.html">
     8.4. Review Problems: Set 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_09/00_Conditioning_Revisited.html">
   9. Conditioning, Revisited
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/01_Probability_by_Conditioning.html">
     9.1. Probability by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/02_Expectation_by_Conditioning.html">
     9.2. Expectation by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_09/03_Expected_Waiting_Times.html">
     9.3. Expected Waiting Times
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_10/00_Markov_Chains.html">
   10. Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/01_Transitions.html">
     10.1. Transitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/02_Deconstructing_Chains.html">
     10.2. Deconstructing Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/03_Long_Run_Behavior.html">
     10.3. Long Run Behavior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_10/04_Examples.html">
     10.4. Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_11/00_Reversing_Markov_Chains.html">
   11. Reversing Markov Chains
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/01_Detailed_Balance.html">
     11.1. Detailed Balance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/02_Reversibility.html">
     11.2. Reversibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/03_Code_Breaking.html">
     11.3. Code Breaking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/04_Markov_Chain_Monte_Carlo.html">
     11.4. Markov Chain Monte Carlo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_11/05_Review_Conditioning_and_MC.html">
     11.5. Review Set on Conditioning and Markov Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_12/00_Standard_Deviation.html">
   12. Standard Deviation
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/01_Definition.html">
     12.1. Definition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/02_Prediction_and_Estimation.html">
     12.2. Prediction and Estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/03_Bounds.html">
     12.3. Tail Bounds
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_12/04_Heavy_Tails.html">
     12.4. Heavy Tails
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_13/00_Variance_Via_Covariance.html">
   13. Variance Via Covariance
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/01_Properties_of_Covariance.html">
     13.1. Properties of Covariance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/02_Sums_of_IID_Samples.html">
     13.2. Sums of IID Samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/03_Sums_of_Simple_Random_Samples.html">
     13.3. Sums of Simple Random Samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_13/04_Finite_Population_Correction.html">
     13.4. Finite Population Correction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_14/00_The_Central_Limit_Theorem.html">
   14. The Central Limit Theorem
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/01_Exact_Distribution.html">
     14.1. Exact Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/02_PGFs_in_NumPy.html">
     14.2. PGFs in NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/03_Central_Limit_Theorem.html">
     14.3. Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/04_The_Sample_Mean.html">
     14.4. The Sample Mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_14/05_Confidence_Intervals.html">
     14.5. Confidence Intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_15/00_Continuous_Distributions.html">
   15. Continuous Distributions
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/01_Density_and_CDF.html">
     15.1. Density and CDF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/02_The_Meaning_of_Density.html">
     15.2. The Meaning of Density
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/03_Expectation.html">
     15.3. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/04_Exponential_Distribution.html">
     15.4. Exponential Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/05_Calculus_in_SymPy.html">
     15.5. Calculus in SymPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_15/06_Review_Problems_Set_3.html">
     15.6. Review Problems: Set 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_16/00_Transformations.html">
   16. Transformations
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/01_Linear_Transformations.html">
     16.1. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/02_Monotone_Functions.html">
     16.2. Monotone Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_16/03_Two_to_One_Functions.html">
     16.3. Two-to-One Functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_17/00_Joint_Densities.html">
   17. Joint Densities
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/01_Probabilities_and_Expectations.html">
     17.1. Probabilities and Expectations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/02_Independence.html">
     17.2. Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/03_Marginal_and_Conditional_Densities.html">
     17.3. Marginal and Conditional Densities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_17/04_Beta_Densities_with_Integer_Parameters.html">
     17.4. Beta Densities with Integer Parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_18/00_The_Normal_and_Gamma_Families.html">
   18. The Normal and Gamma Families
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/01_Standard_Normal_Basics.html">
     18.1. Standard Normal: The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/02_Sums_of_Independent_Normal_Variables.html">
     18.2. Sums of Independent Normal Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/03_The_Gamma_Family.html">
     18.3. The Gamma Family
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/04_Chi_Squared_Distributions.html">
     18.4. Chi-Squared Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_18/05_Review_Problems_Set_4.html">
     18.5. Review Problems: Set 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_19/00_Distributions_of_Sums.html">
   19. Distributions of Sums
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/01_Convolution_Formula.html">
     19.1. The Convolution Formula
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/02_Moment_Generating_Functions.html">
     19.2. Moment Generating Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/03_MGFs_Normal_and_the_CLT.html">
     19.3. MGFs, the Normal, and the CLT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_19/04_Chernoff_Bound.html">
     19.4. Chernoff Bound
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="00_Approaches_to_Estimation.html">
   20. Approaches to Estimation
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     20.1. Maximum Likelihood##
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_Prior_and_Posterior.html">
     20.2. Prior and Posterior
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_Independence_Revisited.html">
     20.3. Independence, Revisited
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_21/00_The_Beta_and_the_Binomial.html">
   21. The Beta and the Binomial
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/01_Updating_and_Prediction.html">
     21.1. Updating and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/02_Beta_Binomial_Distribution.html">
     21.2. The Beta-Binomial Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_21/03_Long_Run_Proportion_of_Heads.html">
     21.3. Long Run Proportion of Heads
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_22/00_Prediction.html">
   22. Prediction
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/01_Conditional_Expectation_Projection.html">
     22.1. Conditional Expectation As a Projection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/02_Variance_by_Conditioning.html">
     22.2. Variance by Conditioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/03_Examples.html">
     22.3. Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_22/04_Least_Squares_Predictor.html">
     22.4. Least Squares Predictor
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_23/00_Multivariate_Normal_RVs.html">
   23. Jointly Normal Random Variables
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/01_Random_Vectors.html">
     23.1. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/02_Multivariate_Normal_Distribution.html">
     23.2. Multivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/03_Linear_Combinations.html">
     23.3. Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_23/04_Independence.html">
     23.4. Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_24/00_Simple_Linear_Regression.html">
   24. Simple Linear Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/01_Bivariate_Normal_Distribution.html">
     24.1. Bivariate Normal Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/02_Linear_Least_Squares.html">
     24.2. Least Squares Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/03_Regression_and_Bivariate_Normal.html">
     24.3. Regression and the Bivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_24/04_Regression_Equation.html">
     24.4. The Regression Equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter_25/00_Multiple_Regression.html">
   25. Multiple Regression
  </a>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/01_Bilinearity_in_Matrix_Notation.html">
     25.1. Bilinearity in Matrix Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/02_Best_Linear_Predictor.html">
     25.2. Best Linear Predictor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/03_Multivariate_Normal_Conditioning.html">
     25.3. Conditioning and the Multivariate Normal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/04_Multiple_Regression.html">
     25.4. Multiple Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter_25/05_Further_Review_Exercises.html">
     25.5. Further Review Exercises
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/Chapter_20/01_Maximum_Likelihood.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://prob140.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/textbook&urlpath=tree/textbook/content/Chapter_20/01_Maximum_Likelihood.ipynb&branch=gh-pages"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> On this page
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample">
   20.1.1. MLE of
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   Based on a Normal
   <span class="math notranslate nohighlight">
    \((\mu, \sigma^2)\)
   </span>
   Sample
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-likelihood-function">
     20.1.1.1. The Likelihood Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-log-likelihood-function">
     20.1.1.2. The Log Likelihood Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivative-of-the-log-likelihood-function">
     20.1.1.3. Derivative of the Log Likelihood Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#set-equal-to-0-and-solve-for-the-mle">
     20.1.1.4. Set Equal to 0 and Solve for the MLE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steps-for-finding-the-mle">
   20.1.2. Steps for Finding the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mle-of-p-based-on-a-bernoulli-p-sample">
   20.1.3. MLE of
   <span class="math notranslate nohighlight">
    \(p\)
   </span>
   Based on a Bernoulli
   <span class="math notranslate nohighlight">
    \((p)\)
   </span>
   Sample
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#properties-of-the-mle">
   20.1.4. Properties of the MLE
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">
   20.1.5. MLEs of
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   and
   <span class="math notranslate nohighlight">
    \(\sigma\)
   </span>
   Based on a Normal
   <span class="math notranslate nohighlight">
    \((\mu, \sigma^2)\)
   </span>
   Sample
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     20.1.5.1. The Likelihood Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     20.1.5.2. The Log Likelihood Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximizing-the-log-likelihood-function">
     20.1.5.3. Maximizing the Log Likelihood Function
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="maximum-likelihood">
<h1><span class="section-number">20.1. </span>Maximum Likelihood##<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h1>
<p>Suppose you have an i.i.d. sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> where the density of each <span class="math notranslate nohighlight">\(X_i\)</span> depends on a parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Assume that <span class="math notranslate nohighlight">\(\theta\)</span> is fixed but unknown. The method of <em>maximum likelihood</em> estimates <span class="math notranslate nohighlight">\(\theta\)</span> by answering the following question:</p>
<p><strong>Among all the possible values of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which one maximizes the likeihood of getting our sample?</strong></p>
<p>That maximizing value of the parameter is called the <em>maximum likelihood estimate</em> or MLE for short. In this section we will develop a method for finding MLEs.</p>
<p>Let’s look at an example to illustrate the main idea. Suppose you know that your sample is drawn from the normal <span class="math notranslate nohighlight">\((\mu, 1)\)</span> distribution for an unknown <span class="math notranslate nohighlight">\(\mu\)</span>, and you are trying to estimate the value of <span class="math notranslate nohighlight">\(\mu\)</span>. Suppose the sampled values are 52.8, 51.1, 54.2, and 52.5.</p>
<p>That’s a small sample but it carries information. If you had to choose between 32 and 52 as values for <span class="math notranslate nohighlight">\(\mu\)</span>, which would you choose?</p>
<p>Without any detailed calculations it’s clear that 32 is not a good choice – the normal <span class="math notranslate nohighlight">\((32, 1)\)</span> distribution is unlikely to produce values as large as those in the observed sample. If 32 and 52 are your only two choices for <span class="math notranslate nohighlight">\(\mu\)</span>, you should choose 52.</p>
<p>But of course <span class="math notranslate nohighlight">\(\mu\)</span> could be any number. To find the best one, you do have to do a calculation.</p>
<div class="section" id="mle-of-mu-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.1. </span>MLE of <span class="math notranslate nohighlight">\(\mu\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mle-of-mu-based-on-a-normal-mu-sigma-2-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. The sample mean is a pretty good estimate of <span class="math notranslate nohighlight">\(\mu\)</span>, as you know. In this example we will show that it is the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>What if you want to estimate <span class="math notranslate nohighlight">\(\sigma\)</span> as well? We will tackle that problem at the end of this section. For now, let’s just estimate <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<div class="section" id="the-likelihood-function">
<h3><span class="section-number">20.1.1.1. </span>The Likelihood Function<a class="headerlink" href="#the-likelihood-function" title="Permalink to this headline">¶</a></h3>
<p>The <em>likelihood function</em> is the joint density of the sample evaluated at the observed values, considered as a function of the parameter. That’s a bit of a mouthful but it becomes clear once you see the calculation. The joint density in this example is the product of <span class="math notranslate nohighlight">\(n\)</span> normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> density functions, and hence the likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)}
\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is called the likelihood of the data <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> when the mean of the underlying normal distribution is <span class="math notranslate nohighlight">\(\mu\)</span>. For every fixed <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(Lik(\mu)\)</span> is a function of the sample and hence is a random variable.</p>
<p>You’ll soon see the reason for using the strange notation <span class="math notranslate nohighlight">\(Lik\)</span>. Please just accept it for now.</p>
<p>The goal is to find the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes this likelihood function over all the possible values that <span class="math notranslate nohighlight">\(\mu\)</span> could be. We don’t yet know if such a maximizing value exists, but let’s try to find it anyway.</p>
<p>To do this we will simplify the likelihood function as much as possible.</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu) ~ = ~ \big{(} \frac{1}{\sqrt{2\pi}\sigma} \big{)}^n
\exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
~ = ~ C \exp \big{(} -\frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\mu\)</span> and thus won’t affect the maximization.</p>
<p>Even in this simplified form, the likelihood function looks difficult to maximize. But as it is a product, we can simplify our calculations still further by taking its log.</p>
</div>
<div class="section" id="the-log-likelihood-function">
<h3><span class="section-number">20.1.1.2. </span>The Log Likelihood Function<a class="headerlink" href="#the-log-likelihood-function" title="Permalink to this headline">¶</a></h3>
<p>Not only does the log function turn products into sums, it is an increasing function. Hence <strong>the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes the likelihood function is the same as the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes the log of the likelihood function.</strong></p>
<p>Let <span class="math notranslate nohighlight">\(L\)</span> be the log of the likelihood function, also known as the <em>log likelihood function</em>. You can see the letter l appearing repeatedly in the terminology. Since we’ll be doing most of our work with the log likelihood function, we are calling it <span class="math notranslate nohighlight">\(L\)</span> and using <span class="math notranslate nohighlight">\(Lik\)</span> for the likelihood function.</p>
<div class="math notranslate nohighlight">
\[
L(\mu) ~ = ~ \log(C) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
<p>The function <span class="math notranslate nohighlight">\(L\)</span> looks much more friendly than <span class="math notranslate nohighlight">\(Lik\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(\log(C)\)</span> doesn’t affect the maximization, we have defined a function to calculate <span class="math notranslate nohighlight">\(L - \log(C)\)</span> for the sample 52.8, 51.1, 54.2, and 52.5 drawn from the normal <span class="math notranslate nohighlight">\((\mu, 1)\)</span> distribution. Remember that we began this section by comparing 32 and 52 as estimates of <span class="math notranslate nohighlight">\(\mu\)</span>, based on this sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">(</span><span class="mf">52.8</span><span class="p">,</span> <span class="mf">51.1</span><span class="p">,</span> <span class="mf">54.2</span><span class="p">,</span> <span class="mf">52.5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">shifted_log_lik</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">((</span><span class="n">sample</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a graph of the function for <span class="math notranslate nohighlight">\(\mu\)</span> in the interval <span class="math notranslate nohighlight">\((30, 70)\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/01_Maximum_Likelihood_6_0.png" src="../../_images/01_Maximum_Likelihood_6_0.png" />
</div>
</div>
<p>The maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> looks very close to 52.5. To find exactly where it is, we will find the derivative of <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and set that equal to 0.</p>
</div>
<div class="section" id="derivative-of-the-log-likelihood-function">
<h3><span class="section-number">20.1.1.3. </span>Derivative of the Log Likelihood Function<a class="headerlink" href="#derivative-of-the-log-likelihood-function" title="Permalink to this headline">¶</a></h3>
<p>Use the Chain Rule and be careful about negative signs.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\mu} L(\mu) ~ = ~ \frac{2}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)
\]</div>
</div>
<div class="section" id="set-equal-to-0-and-solve-for-the-mle">
<h3><span class="section-number">20.1.1.4. </span>Set Equal to 0 and Solve for the MLE<a class="headerlink" href="#set-equal-to-0-and-solve-for-the-mle" title="Permalink to this headline">¶</a></h3>
<p>Statisticians have long used the “hat” symbol to denote estimates. So let <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> be the MLE of <span class="math notranslate nohighlight">\(\mu\)</span>. Then <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> satisfies an equation:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (X_i - \hat{\mu}) ~ = ~ 0 ~~~~~~ \Longleftrightarrow ~~~~~~ \sum_{i=1}^n X_i ~ = ~ n\hat{\mu} ~~~~~~ \Longleftrightarrow ~~~~~~ \hat{\mu} ~ = ~ \frac{1}{n} \sum_{i=1}^n X_i ~ = ~ \bar{X}
\]</div>
<p>We should check that this yields a maximum and not a minimum, but given the answer you will surely accept that it’s a max. You are welcome to take the second derivative of <span class="math notranslate nohighlight">\(L\)</span> and check that we do indeed have a maximum.</p>
<p>We have shown that the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>, regardless of the population SD <span class="math notranslate nohighlight">\(\sigma\)</span>. In the case of the sample we used for the plot above, <span class="math notranslate nohighlight">\(\bar{X} = 52.65\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>52.650000000000006
</pre></div>
</div>
</div>
</div>
<p>You know that the distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is normal with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. If you don’t know <span class="math notranslate nohighlight">\(\sigma\)</span>, then if the sample is large you can estimate <span class="math notranslate nohighlight">\(\sigma\)</span> by the SD of the sample and hence construct confidence intervals for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
</div>
<div class="section" id="steps-for-finding-the-mle">
<h2><span class="section-number">20.1.2. </span>Steps for Finding the MLE<a class="headerlink" href="#steps-for-finding-the-mle" title="Permalink to this headline">¶</a></h2>
<p>Let’s capture our sequence of steps in an algorithm to find the MLE of a parameter given an i.i.d. sample.</p>
<ul class="simple">
<li><p>Write the likelihood of the sample. The goal is to find the value of the parameter that maximizes this likelihood.</p></li>
<li><p>To make the maximization easier, take the log of the likelihood function.</p></li>
<li><p>To maximize the log likelihood with respect to the parameter, take its derivative with respect to the parameter.</p></li>
<li><p>Set the derivative equal to 0 and solve; the solution is the MLE.</p></li>
</ul>
<p>Let’s implement this algorithm in another example.</p>
</div>
<div class="section" id="mle-of-p-based-on-a-bernoulli-p-sample">
<h2><span class="section-number">20.1.3. </span>MLE of <span class="math notranslate nohighlight">\(p\)</span> Based on a Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> Sample<a class="headerlink" href="#mle-of-p-based-on-a-bernoulli-p-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. Bernoulli <span class="math notranslate nohighlight">\((p)\)</span> sample. Our goal is to find the MLE of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>The random variables are discrete, so the likelihood function is defined as the joint probability mass function evaluated at the sample. To see what this means, let’s start with the example.</p>
<p>Suppose <span class="math notranslate nohighlight">\(n=5\)</span> and the observed sequence of 1’s and 0’s is 01101. The likelihood function at <span class="math notranslate nohighlight">\(p\)</span> is the chance of observing this sequence under that value of <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Lik(p) ~ = ~ (1-p)\cdot p \cdot p \cdot (1-p) \cdot p ~ = ~ p^3(1-p)^2
\]</div>
<p>The likelihood depends on the number of 1’s, just as in the binomial probability formula. The combinatorial term is missing because we are observing each element of the sequence.</p>
<p>Now let’s implement our algorithm for finding the MLE.</p>
<p><strong>Step 1: Find the likelihood function.</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X = X_1 + X_2 + \ldots + X_n\)</span> be the number of 1’s in the sample. The likelihood function is</p>
<div class="math notranslate nohighlight">
\[
Lik(p) = p^X (1-p)^{n-X}
\]</div>
<p><strong>Step 2. Find the log likelihood function.</strong></p>
<div class="math notranslate nohighlight">
\[
L(p) = X\log(p) + (n-X)\log(1-p)
\]</div>
<p><strong>Step 3. Find the derivative of the log likelihood function.</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dp} L(p) = \frac{X}{p} - \frac{n-X}{1-p}
\]</div>
<p><strong>Step 4. Set equal to 0 and solve for the MLE.</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{X}{\hat{p}} - \frac{n-X}{1-\hat{p}} = 0
\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
(1-\hat{p})X = (n-X)\hat{p} ~~~~~ \text{so} ~~~~~ X = n\hat{p}
\]</div>
<p>Therefore the MLE of <span class="math notranslate nohighlight">\(p\)</span> is</p>
<div class="math notranslate nohighlight">
\[ 
\hat{p} = \frac{X}{n} = \frac{1}{n}\sum_{i=1}^n X_i
\]</div>
<p>That is, the MLE of <span class="math notranslate nohighlight">\(p\)</span> is the sample proportion of 1’s. To compute this estimate, all you need is the number of 1’s in the sample. You don’t need to see the entire sample as a sequence of 0’s and 1’s.</p>
<p>Because the MLE <span class="math notranslate nohighlight">\(\hat{p}\)</span> is the sample proportion, it is unbiased, has SD <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n}\)</span>, and is asymptotically normal. When <span class="math notranslate nohighlight">\(n\)</span> is large you can estimate the SD based on the sample and therefore construct confidence intervals for <span class="math notranslate nohighlight">\(p\)</span>.</p>
</div>
<div class="section" id="properties-of-the-mle">
<h2><span class="section-number">20.1.4. </span>Properties of the MLE<a class="headerlink" href="#properties-of-the-mle" title="Permalink to this headline">¶</a></h2>
<p>In the two examples above, the MLE is unbiased and either exactly normal or asymptotically normal. In general, MLEs need not be unbiased, as you will see in an example below. However, under some regularity conditions on the underlying probability distribution or mass function, when the sample is large the MLE is:</p>
<ul class="simple">
<li><p><em>consistent</em>, that is, likely to be close to the parameter</p></li>
<li><p>roughly normal and almost unbiased</p></li>
</ul>
<p>Establishing this is outside the scope of this class, but in exercises you will observe these properties through simulation.</p>
<p>Though there is beautiful theory about the asymptotic variance of the MLE, in practice it can be hard to estimate the variance analytically. This can make it hard to find formulas for confidence intervals. However, you can use the bootstrap to estimate the variance: each bootstrapped sample yields a value of the MLE, and you can construct confidence intervals based on the empirical distribution of the bootstrapped MLEs.</p>
</div>
<div class="section" id="mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample">
<h2><span class="section-number">20.1.5. </span>MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> Based on a Normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> Sample<a class="headerlink" href="#mles-of-mu-and-sigma-based-on-a-normal-mu-sigma-2-sample" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample. We will now find the MLEs of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<div class="section" id="id1">
<h3><span class="section-number">20.1.5.1. </span>The Likelihood Function<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>We have to think of this as a function of both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Lik(\mu, \sigma) ~ = ~ \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \big{(} -\frac{1}{2} \big{(} \frac{X_i - \mu}{\sigma} \big{)}^2 \big{)} ~ = ~
C \cdot \frac{1}{\sigma^n} \prod_{i=1}^n \exp \big{(} -\frac{1}{2\sigma^2} (X_i - \mu)^2 \big{)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(C = 1/(\sqrt{2\pi})^n\)</span> does not affect the maximization.</p>
</div>
<div class="section" id="id2">
<h3><span class="section-number">20.1.5.2. </span>The Log Likelihood Function<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[
L(\mu, \sigma) ~ = ~ \log(C) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \mu)^2
\]</div>
</div>
<div class="section" id="maximizing-the-log-likelihood-function">
<h3><span class="section-number">20.1.5.3. </span>Maximizing the Log Likelihood Function<a class="headerlink" href="#maximizing-the-log-likelihood-function" title="Permalink to this headline">¶</a></h3>
<p>We will maximize <span class="math notranslate nohighlight">\(L\)</span> in two stages:</p>
<ul class="simple">
<li><p>First fix <span class="math notranslate nohighlight">\(\sigma\)</span> and maximize with respect to <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Then plug in the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> and maximize the resulting function with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ul>
<p>We have already completed the first stage in the first example of this section. For each fixed <span class="math notranslate nohighlight">\(\sigma\)</span>, the maximizing value of <span class="math notranslate nohighlight">\(\mu\)</span> is <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>.</p>
<p>So now our job is to find the value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> that maximizes the new function</p>
<div class="math notranslate nohighlight">
\[
L^*(\sigma) ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (X_i - \bar{X})^2 ~ = ~ -n\log(\sigma) - \frac{1}{2\sigma^2} V
\]</div>
<p>where <span class="math notranslate nohighlight">\(V = \sum_{i=1}^n (X_i - \bar{X})^2\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\sigma\)</span>. Differentiate with respect to <span class="math notranslate nohighlight">\(\sigma\)</span>; keep track of minus signs and factors of 2.</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\sigma} L^*(\sigma) ~ = ~ -\frac{n}{\sigma} + \frac{1}{\sigma^3}V
\]</div>
<p>Set this equal to 0 and solve for the maximizing value <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
-\frac{n}{\hat{\sigma}} + \frac{1}{\hat{\sigma}^3}V ~ = ~ 0 
~~~~~~~ \Longleftrightarrow ~~~~~~~ \hat{\sigma}^2 ~ = ~ \frac{V}{n} ~ = ~ 
\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</div>
<p>Again you should check that this yields a maximum and not a minimum, but again given the answer you will surely accept that it’s a max.</p>
<p>You have shown in exercises that <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is <em>not</em> an unbiased estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>. You have also shown that it is close to unbiased when <span class="math notranslate nohighlight">\(n\)</span> is large.</p>
<p>To summarize our result, if <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> is an i.i.d. normal <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span> sample, then the MLEs of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma} = \sqrt{\hat{\sigma}^2}\)</span> where <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\)</span></p></li>
</ul>
<p>It is a remarkable fact about i.i.d. normal samples that <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> are independent of each other even though they are statistics calculated from the same sample. Later in this course you will see why.</p>
<p><strong>Computational Note:</strong> MLEs can’t always be derived analytically as easily as in our examples. It’s important to keep in mind that maximizing log likelihood functions can often be intractable without a numerical optimization method. Also, not all likelihood functions have unique maxima.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/Chapter_20"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="00_Approaches_to_Estimation.html" title="previous page"><span class="section-number">20. </span>Approaches to Estimation</a>
    <a class='right-next' id="next-link" href="02_Prior_and_Posterior.html" title="next page"><span class="section-number">20.2. </span>Prior and Posterior</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ani Adhikari<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>